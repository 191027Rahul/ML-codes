{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "1e10951b-8f96-4826-b432-92c937d1d51c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature selection completed successfully!\n",
      "Top  energy-related precursors:\n",
      "                Selected Features  MI Score\n",
      "0               max_magnitude_new  1.078552\n",
      "1       magnitude_deficit_lsq_new  0.330083\n",
      "2   Earthquake greater than 4_old  0.281494\n",
      "3       magnitude_deficit_mlk_new  0.249386\n",
      "4      Smoothed Time Since EQ_old  0.206465\n",
      "5        probabilities_m6_lsq_new  0.204380\n",
      "6        probabilities_m6_mlk_new  0.178276\n",
      "7                seismic_rate_new  0.172723\n",
      "8   Combined Strain Rate Grid_old  0.129685\n",
      "9             Fault Distances_old  0.113822\n",
      "10                b_value_mlk_new  0.087389\n",
      "11                b_value_lsq_new  0.083017\n",
      "12             seismic_energy_new  0.078538\n",
      "13    recurrence_time_4.8_mlk_new  0.078247\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.feature_selection import mutual_info_regression\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "import openpyxl\n",
    "\n",
    "# Load the dataset\n",
    "file_path = \"Flattened_Seismic_Data_With_Ranges.xlsx\"\n",
    "df = pd.read_excel(file_path)\n",
    "\n",
    "# Updated precursor parameters based on your research needs\n",
    "all_columns = [\n",
    "    \"Earthquake greater than 4_old\",\n",
    "    \"Fault Density_old\",\n",
    "    \"Fault Distances_old\",\n",
    "    \"Combined Strain Rate Grid_old\",\n",
    "   \"seismic_energy_new\",\n",
    "    \"Smoothed Time Since EQ_old\",\n",
    "    \"b_value_lsq_new\",\n",
    "    \"b_value_mlk_new\",\n",
    "    \"probabilities_m6_lsq_new\",\n",
    "    \"probabilities_m6_mlk_new\",\n",
    "    \"magnitude_deficit_lsq_new\",\n",
    "    \"magnitude_deficit_mlk_new\",\n",
    "    \"seismic_rate_new\",\n",
    "   \n",
    "    \"max_magnitude_new\",\n",
    "    \n",
    "    \"recurrence_time_4.0_lsq_new\",\n",
    "    \"recurrence_time_4.0_mlk_new\",\n",
    "    \"recurrence_time_4.1_lsq_new\",\n",
    "    \"recurrence_time_4.1_mlk_new\",\n",
    "    \"recurrence_time_4.2_lsq_new\",\n",
    "    \"recurrence_time_4.2_mlk_new\",\n",
    "    \"recurrence_time_4.3_lsq_new\",\n",
    "    \"recurrence_time_4.3_mlk_new\",\n",
    "    \"recurrence_time_4.4_lsq_new\",\n",
    "    \"recurrence_time_4.4_mlk_new\",\n",
    "    \"recurrence_time_4.5_lsq_new\",\n",
    "    \"recurrence_time_4.5_mlk_new\",\n",
    "    \"recurrence_time_4.6_lsq_new\",\n",
    "    \"recurrence_time_4.6_mlk_new\",\n",
    "    \"recurrence_time_4.7_lsq_new\",\n",
    "    \"recurrence_time_4.7_mlk_new\",\n",
    "    \"recurrence_time_4.8_lsq_new\",\n",
    "    \"recurrence_time_4.8_mlk_new\",\n",
    "    \"recurrence_time_4.9_lsq_new\",\n",
    "    \"recurrence_time_4.9_mlk_new\",\n",
    "    \"recurrence_time_5.0_lsq_new\",\n",
    "    \"recurrence_time_5.0_mlk_new\",\n",
    "    \"recurrence_time_5.1_lsq_new\",\n",
    "    \"recurrence_time_5.1_mlk_new\",\n",
    "    \"recurrence_time_5.2_lsq_new\",\n",
    "    \"recurrence_time_5.2_mlk_new\",\n",
    "    \"recurrence_time_5.3_lsq_new\",\n",
    "    \"recurrence_time_5.3_mlk_new\",\n",
    "    \"recurrence_time_5.4_lsq_new\",\n",
    "    \"recurrence_time_5.4_mlk_new\",\n",
    "    \"recurrence_time_5.5_lsq_new\",\n",
    "    \"recurrence_time_5.5_mlk_new\",\n",
    "    \"recurrence_time_5.6_lsq_new\",\n",
    "    \"recurrence_time_5.6_mlk_new\",\n",
    "    \"recurrence_time_5.7_lsq_new\",\n",
    "    \"recurrence_time_5.7_mlk_new\",\n",
    "    \"recurrence_time_5.8_lsq_new\",\n",
    "    \"recurrence_time_5.8_mlk_new\",\n",
    "    \"recurrence_time_5.9_lsq_new\",\n",
    "    \"recurrence_time_5.9_mlk_new\",\n",
    "    \"recurrence_time_6.0_lsq_new\",\n",
    "    \"recurrence_time_6.0_mlk_new\"\n",
    "]\n",
    "\n",
    "# Updated target variable\n",
    "target_column =  \"mean_magnitude_new\"\n",
    "\n",
    "# Data cleaning and preprocessing\n",
    "df_clean = df[all_columns + [target_column]]\n",
    "df_clean = df_clean.apply(pd.to_numeric, errors='coerce')\n",
    "\n",
    "# Separate features and target\n",
    "X = df_clean[all_columns]\n",
    "y = df_clean[target_column]\n",
    "\n",
    "# Feature selection process\n",
    "mi_scores = mutual_info_regression(X, y)\n",
    "mi_results = pd.DataFrame({'Feature': X.columns, 'Mutual_Info_Score': mi_scores})\n",
    "mi_results = mi_results.sort_values(by='Mutual_Info_Score', ascending=False)\n",
    "\n",
    "# Threshold selection (using median instead of mean for skewed distributions)\n",
    "mi_threshold = mi_results['Mutual_Info_Score'].median()\n",
    "selected_features = mi_results[mi_results['Mutual_Info_Score'] >= mi_threshold]['Feature'].tolist()\n",
    "\n",
    "# Enhanced redundancy check (considering energy-specific relationships)\n",
    "redundancy_threshold = 2.0\n",
    "redundant_features = set()\n",
    "\n",
    "# Check pairwise MI between selected features\n",
    "for i, feature1 in enumerate(selected_features):\n",
    "    for feature2 in selected_features[i+1:]:\n",
    "        mi_value = mutual_info_regression(X[[feature1]], X[feature2])[0]\n",
    "        if mi_value >= redundancy_threshold:\n",
    "            # Keep the feature with higher MI to target\n",
    "            if mi_results.loc[mi_results['Feature'] == feature1, 'Mutual_Info_Score'].values[0] > \\\n",
    "               mi_results.loc[mi_results['Feature'] == feature2, 'Mutual_Info_Score'].values[0]:\n",
    "                redundant_features.add(feature2)\n",
    "            else:\n",
    "                redundant_features.add(feature1)\n",
    "\n",
    "final_features = [f for f in selected_features if f not in redundant_features]\n",
    "\n",
    "# Save results with improved formatting\n",
    "# Save results with improved formatting\n",
    "excel_path = \"Seismic_Energy_Precursor_Selection.xlsx\"\n",
    "with pd.ExcelWriter(excel_path, engine=\"openpyxl\") as writer:\n",
    "    mi_results.to_excel(writer, sheet_name=\"Mutual Information\", index=False)\n",
    "    \n",
    "    summary_df = pd.DataFrame({\n",
    "        'Selected Features': final_features,\n",
    "        'MI Score': [mi_results.loc[mi_results['Feature'] == f, 'Mutual_Info_Score'].values[0] \n",
    "                     for f in final_features]  # Added closing bracket here\n",
    "    })\n",
    "    summary_df.to_excel(writer, sheet_name=\"Final Features\", index=False)\n",
    "\n",
    "print(\"Feature selection completed successfully!\")\n",
    "print(f\"Top  energy-related precursors:\\n{summary_df.head(15)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "542fccdb-6c34-4b42-be68-006d8ab89b21",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature selection completed successfully using SHAP!\n",
      "Top energy-related precursors based on SHAP values:\n",
      "                Selected Features  Mean SHAP Value\n",
      "0               max_magnitude_new         0.318153\n",
      "1                seismic_rate_new         0.011462\n",
      "2       magnitude_deficit_mlk_new         0.010352\n",
      "3   Combined Strain Rate Grid_old         0.006967\n",
      "4   Earthquake greater than 4_old         0.005330\n",
      "5       magnitude_deficit_lsq_new         0.004534\n",
      "6              seismic_energy_new         0.003917\n",
      "7             Fault Distances_old         0.003228\n",
      "8      Smoothed Time Since EQ_old         0.002419\n",
      "9        probabilities_m6_lsq_new         0.001095\n",
      "10                b_value_lsq_new         0.000975\n",
      "11       probabilities_m6_mlk_new         0.000844\n",
      "12                b_value_mlk_new         0.000560\n",
      "13    recurrence_time_4.0_lsq_new         0.000557\n",
      "14              Fault Density_old         0.000494\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import shap\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import openpyxl\n",
    "\n",
    "# Load the dataset\n",
    "file_path = \"Flattened_Seismic_Data_With_Ranges.xlsx\"\n",
    "df = pd.read_excel(file_path)\n",
    "\n",
    "# Updated precursor parameters based on your research needs\n",
    "all_columns = [\n",
    "    \"Earthquake greater than 4_old\",\n",
    "    \"Fault Density_old\",\n",
    "    \"Fault Distances_old\",\n",
    "    \"Combined Strain Rate Grid_old\",\n",
    "    \"seismic_energy_new\",\n",
    "    \"Smoothed Time Since EQ_old\",\n",
    "    \"b_value_lsq_new\",\n",
    "    \"b_value_mlk_new\",\n",
    "    \"probabilities_m6_lsq_new\",\n",
    "    \"probabilities_m6_mlk_new\",\n",
    "    \"magnitude_deficit_lsq_new\",\n",
    "    \"magnitude_deficit_mlk_new\",\n",
    "    \"seismic_rate_new\",\n",
    "    \"max_magnitude_new\",\n",
    "    \"recurrence_time_4.0_lsq_new\",\n",
    "    \"recurrence_time_4.0_mlk_new\",\n",
    "    \"recurrence_time_4.1_lsq_new\",\n",
    "    \"recurrence_time_4.1_mlk_new\",\n",
    "    \"recurrence_time_4.2_lsq_new\",\n",
    "    \"recurrence_time_4.2_mlk_new\",\n",
    "    \"recurrence_time_4.3_lsq_new\",\n",
    "    \"recurrence_time_4.3_mlk_new\",\n",
    "    \"recurrence_time_4.4_lsq_new\",\n",
    "    \"recurrence_time_4.4_mlk_new\",\n",
    "    \"recurrence_time_4.5_lsq_new\",\n",
    "    \"recurrence_time_4.5_mlk_new\",\n",
    "    \"recurrence_time_4.6_lsq_new\",\n",
    "    \"recurrence_time_4.6_mlk_new\",\n",
    "    \"recurrence_time_4.7_lsq_new\",\n",
    "    \"recurrence_time_4.7_mlk_new\",\n",
    "    \"recurrence_time_4.8_lsq_new\",\n",
    "    \"recurrence_time_4.8_mlk_new\",\n",
    "    \"recurrence_time_4.9_lsq_new\",\n",
    "    \"recurrence_time_4.9_mlk_new\",\n",
    "    \"recurrence_time_5.0_lsq_new\",\n",
    "    \"recurrence_time_5.0_mlk_new\",\n",
    "    \"recurrence_time_5.1_lsq_new\",\n",
    "    \"recurrence_time_5.1_mlk_new\",\n",
    "    \"recurrence_time_5.2_lsq_new\",\n",
    "    \"recurrence_time_5.2_mlk_new\",\n",
    "    \"recurrence_time_5.3_lsq_new\",\n",
    "    \"recurrence_time_5.3_mlk_new\",\n",
    "    \"recurrence_time_5.4_lsq_new\",\n",
    "    \"recurrence_time_5.4_mlk_new\",\n",
    "    \"recurrence_time_5.5_lsq_new\",\n",
    "    \"recurrence_time_5.5_mlk_new\",\n",
    "    \"recurrence_time_5.6_lsq_new\",\n",
    "    \"recurrence_time_5.6_mlk_new\",\n",
    "    \"recurrence_time_5.7_lsq_new\",\n",
    "    \"recurrence_time_5.7_mlk_new\",\n",
    "    \"recurrence_time_5.8_lsq_new\",\n",
    "    \"recurrence_time_5.8_mlk_new\",\n",
    "    \"recurrence_time_5.9_lsq_new\",\n",
    "    \"recurrence_time_5.9_mlk_new\",\n",
    "    \"recurrence_time_6.0_lsq_new\",\n",
    "    \"recurrence_time_6.0_mlk_new\"\n",
    "]\n",
    "\n",
    "# Updated target variable\n",
    "target_column = \"mean_magnitude_new\"\n",
    "\n",
    "# Data cleaning and preprocessing\n",
    "df_clean = df[all_columns + [target_column]]\n",
    "df_clean = df_clean.apply(pd.to_numeric, errors='coerce')\n",
    "\n",
    "# Separate features and target\n",
    "X = df_clean[all_columns]\n",
    "y = df_clean[target_column]\n",
    "\n",
    "# Train a Random Forest model (or any model you prefer)\n",
    "model = RandomForestRegressor(n_estimators=100, random_state=42)\n",
    "model.fit(X, y)\n",
    "\n",
    "# SHAP value calculation\n",
    "explainer = shap.TreeExplainer(model)\n",
    "shap_values = explainer.shap_values(X)\n",
    "\n",
    "# Create a DataFrame for SHAP values\n",
    "shap_df = pd.DataFrame(shap_values, columns=X.columns)\n",
    "\n",
    "shap_abs_mean = shap_df.abs().mean()\n",
    "shap_sorted = shap_abs_mean.sort_values(ascending=False)\n",
    "shap_threshold = shap_sorted.median()\n",
    "selected_features = shap_sorted[shap_sorted >= shap_threshold].index.tolist()\n",
    "\n",
    "redundancy_threshold = 2.0\n",
    "redundant_features = set()\n",
    "\n",
    "# Check pairwise SHAP values between selected features (using a simple correlation method)\n",
    "for i, feature1 in enumerate(selected_features):\n",
    "    for feature2 in selected_features[i+1:]:\n",
    "        corr_value = np.corrcoef(shap_df[feature1], shap_df[feature2])[0, 1]\n",
    "        if abs(corr_value) >= redundancy_threshold:\n",
    "            # Keep the feature with higher SHAP value mean\n",
    "            if shap_abs_mean[feature1] > shap_abs_mean[feature2]:\n",
    "                redundant_features.add(feature2)\n",
    "            else:\n",
    "                redundant_features.add(feature1)\n",
    "\n",
    "final_features = [f for f in selected_features if f not in redundant_features]\n",
    "\n",
    "# Save results with improved formatting\n",
    "excel_path = \"Seismic_Energy_Precursor_Selection_SHAP.xlsx\"\n",
    "with pd.ExcelWriter(excel_path, engine=\"openpyxl\") as writer:\n",
    "    shap_sorted.to_excel(writer, sheet_name=\"SHAP Values\", index=True)\n",
    "    \n",
    "    summary_df = pd.DataFrame({\n",
    "        'Selected Features': final_features,\n",
    "        'Mean SHAP Value': [shap_abs_mean[f] for f in final_features]\n",
    "    })\n",
    "    summary_df.to_excel(writer, sheet_name=\"Final Features\", index=False)\n",
    "\n",
    "print(\"Feature selection completed successfully using SHAP!\")\n",
    "print(f\"Top energy-related precursors based on SHAP values:\\n{summary_df.head(15)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7a2c3806-7053-43aa-bb57-2341a742f315",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature selection completed using Permutation Importance!\n",
      "Top features:\n",
      "                          Feature  Importance Mean  Importance Std\n",
      "13              max_magnitude_new         1.394275        0.038703\n",
      "4   Combined Strain Rate Grid_old         0.067129        0.005219\n",
      "8        probabilities_m6_lsq_new         0.051765        0.004676\n",
      "1              mean_magnitude_new         0.046215        0.003882\n",
      "6                 b_value_lsq_new         0.032892        0.002391\n",
      "5      Smoothed Time Since EQ_old         0.031975        0.003275\n",
      "11      magnitude_deficit_mlk_new         0.027193        0.002616\n",
      "0   Earthquake greater than 4_old         0.020376        0.003020\n",
      "12               seismic_rate_new         0.019332        0.001635\n",
      "10      magnitude_deficit_lsq_new         0.012467        0.000606\n",
      "3             Fault Distances_old         0.007716        0.000725\n",
      "2               Fault Density_old         0.003842        0.000617\n",
      "9        probabilities_m6_mlk_new         0.001571        0.000339\n",
      "7                 b_value_mlk_new         0.001517        0.000428\n",
      "55    recurrence_time_6.0_mlk_new         0.001235        0.000136\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import shap\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.inspection import permutation_importance\n",
    "import openpyxl\n",
    "\n",
    "# Load the dataset\n",
    "file_path = \"Flattened_Seismic_Data_With_Ranges.xlsx\"\n",
    "df = pd.read_excel(file_path)\n",
    "\n",
    "# Updated precursor parameters\n",
    "#3all_columns = [ ... ]  # Same list you already defined earlier\n",
    "all_columns = [\n",
    "    \"Earthquake greater than 4_old\",\n",
    "    \"mean_magnitude_new\",\n",
    "    \"Fault Density_old\",\n",
    "    \"Fault Distances_old\",\n",
    "    \"Combined Strain Rate Grid_old\",\n",
    "    \"Smoothed Time Since EQ_old\",\n",
    "    \"b_value_lsq_new\",\n",
    "    \"b_value_mlk_new\",\n",
    "    \"probabilities_m6_lsq_new\",\n",
    "    \"probabilities_m6_mlk_new\",\n",
    "    \"magnitude_deficit_lsq_new\",\n",
    "    \"magnitude_deficit_mlk_new\",\n",
    "    \"seismic_rate_new\",\n",
    "    \"max_magnitude_new\",\n",
    "    \"recurrence_time_4.0_lsq_new\",\n",
    "    \"recurrence_time_4.0_mlk_new\",\n",
    "    \"recurrence_time_4.1_lsq_new\",\n",
    "    \"recurrence_time_4.1_mlk_new\",\n",
    "    \"recurrence_time_4.2_lsq_new\",\n",
    "    \"recurrence_time_4.2_mlk_new\",\n",
    "    \"recurrence_time_4.3_lsq_new\",\n",
    "    \"recurrence_time_4.3_mlk_new\",\n",
    "    \"recurrence_time_4.4_lsq_new\",\n",
    "    \"recurrence_time_4.4_mlk_new\",\n",
    "    \"recurrence_time_4.5_lsq_new\",\n",
    "    \"recurrence_time_4.5_mlk_new\",\n",
    "    \"recurrence_time_4.6_lsq_new\",\n",
    "    \"recurrence_time_4.6_mlk_new\",\n",
    "    \"recurrence_time_4.7_lsq_new\",\n",
    "    \"recurrence_time_4.7_mlk_new\",\n",
    "    \"recurrence_time_4.8_lsq_new\",\n",
    "    \"recurrence_time_4.8_mlk_new\",\n",
    "    \"recurrence_time_4.9_lsq_new\",\n",
    "    \"recurrence_time_4.9_mlk_new\",\n",
    "    \"recurrence_time_5.0_lsq_new\",\n",
    "    \"recurrence_time_5.0_mlk_new\",\n",
    "    \"recurrence_time_5.1_lsq_new\",\n",
    "    \"recurrence_time_5.1_mlk_new\",\n",
    "    \"recurrence_time_5.2_lsq_new\",\n",
    "    \"recurrence_time_5.2_mlk_new\",\n",
    "    \"recurrence_time_5.3_lsq_new\",\n",
    "    \"recurrence_time_5.3_mlk_new\",\n",
    "    \"recurrence_time_5.4_lsq_new\",\n",
    "    \"recurrence_time_5.4_mlk_new\",\n",
    "    \"recurrence_time_5.5_lsq_new\",\n",
    "    \"recurrence_time_5.5_mlk_new\",\n",
    "    \"recurrence_time_5.6_lsq_new\",\n",
    "    \"recurrence_time_5.6_mlk_new\",\n",
    "    \"recurrence_time_5.7_lsq_new\",\n",
    "    \"recurrence_time_5.7_mlk_new\",\n",
    "    \"recurrence_time_5.8_lsq_new\",\n",
    "    \"recurrence_time_5.8_mlk_new\",\n",
    "    \"recurrence_time_5.9_lsq_new\",\n",
    "    \"recurrence_time_5.9_mlk_new\",\n",
    "    \"recurrence_time_6.0_lsq_new\",\n",
    "    \"recurrence_time_6.0_mlk_new\"\n",
    "]\n",
    "\n",
    "# Target variable\n",
    "target_column = \"seismic_energy_new\"\n",
    "\n",
    "# Clean data\n",
    "df_clean = df[all_columns + [target_column]]\n",
    "df_clean = df_clean.apply(pd.to_numeric, errors='coerce')\n",
    "df_clean = df_clean.dropna()\n",
    "\n",
    "# Separate features and target\n",
    "X = df_clean[all_columns]\n",
    "y = df_clean[target_column]\n",
    "\n",
    "# Train Random Forest model\n",
    "model = RandomForestRegressor(n_estimators=100, random_state=42)\n",
    "model.fit(X, y)\n",
    "\n",
    "# ---------------------------\n",
    "# PERMUTATION FEATURE IMPORTANCE\n",
    "# ---------------------------\n",
    "perm_importance = permutation_importance(model, X, y, n_repeats=10, random_state=42, scoring='r2')\n",
    "\n",
    "# Create DataFrame from permutation results\n",
    "perm_df = pd.DataFrame({\n",
    "    'Feature': X.columns,\n",
    "    'Importance Mean': perm_importance.importances_mean,\n",
    "    'Importance Std': perm_importance.importances_std\n",
    "}).sort_values(by='Importance Mean', ascending=False)\n",
    "\n",
    "# Thresholding (optional): You can set a threshold to filter features\n",
    "median_importance = perm_df['Importance Mean'].median()\n",
    "selected_perm_features = perm_df[perm_df['Importance Mean'] >= median_importance]\n",
    "\n",
    "# Save results to Excel\n",
    "excel_path = \"Seismic_Energy_Precursor_Selection_Permutation.xlsx\"\n",
    "with pd.ExcelWriter(excel_path, engine=\"openpyxl\") as writer:\n",
    "    perm_df.to_excel(writer, sheet_name=\"Permutation Importance\", index=False)\n",
    "    selected_perm_features.to_excel(writer, sheet_name=\"Final Features\", index=False)\n",
    "\n",
    "print(\"Feature selection completed using Permutation Importance!\")\n",
    "print(f\"Top features:\\n{selected_perm_features.head(15)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0fc4acc1-301e-4d41-ab2b-40e226e96fa3",
   "metadata": {},
   "outputs": [],
   "source": [
    "##### LSTM code with attenuation mechanism layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb4b3f95-9b43-4eab-a959-c1d4529a4abb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense, Dropout\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Load Dataset\n",
    "# Load the dataset\n",
    "file_path = \"Flattened_Seismic_Data_With_Ranges.xlsx\"\n",
    "df = pd.read_excel(file_path)\n",
    "\n",
    "# Updated precursor parameters based on your research needs\n",
    "all_columns = [\n",
    "    \"Earthquake greater than 4_old\",\n",
    "    \"Fault Density_old\",\n",
    "    \"Fault Distances_old\",\n",
    "    \"Combined Strain Rate Grid_old\",\n",
    "   \"seismic_energy_new\",\n",
    "    \"Smoothed Time Since EQ_old\",\n",
    "    \"b_value_lsq_new\",\n",
    "    \"b_value_mlk_new\",\n",
    "    \"probabilities_m6_lsq_new\",\n",
    "    \"probabilities_m6_mlk_new\",\n",
    "    \"magnitude_deficit_lsq_new\",\n",
    "    \"magnitude_deficit_mlk_new\",\n",
    "    \"seismic_rate_new\",\n",
    "   \n",
    "    \"max_magnitude_new\",\n",
    "    \n",
    "    \"recurrence_time_4.0_lsq_new\",\n",
    "    \"recurrence_time_4.0_mlk_new\",\n",
    "    \"recurrence_time_4.1_lsq_new\",\n",
    "    \"recurrence_time_4.1_mlk_new\",\n",
    "    \"recurrence_time_4.2_lsq_new\",\n",
    "    \"recurrence_time_4.2_mlk_new\",\n",
    "    \"recurrence_time_4.3_lsq_new\",\n",
    "    \"recurrence_time_4.3_mlk_new\",\n",
    "    \"recurrence_time_4.4_lsq_new\",\n",
    "    \"recurrence_time_4.4_mlk_new\",\n",
    "    \"recurrence_time_4.5_lsq_new\",\n",
    "    \"recurrence_time_4.5_mlk_new\",\n",
    "    \"recurrence_time_4.6_lsq_new\",\n",
    "    \"recurrence_time_4.6_mlk_new\",\n",
    "    \"recurrence_time_4.7_lsq_new\",\n",
    "    \"recurrence_time_4.7_mlk_new\",\n",
    "    \"recurrence_time_4.8_lsq_new\",\n",
    "    \"recurrence_time_4.8_mlk_new\",\n",
    "    \"recurrence_time_4.9_lsq_new\",\n",
    "    \"recurrence_time_4.9_mlk_new\",\n",
    "    \"recurrence_time_5.0_lsq_new\",\n",
    "    \"recurrence_time_5.0_mlk_new\",\n",
    "    \"recurrence_time_5.1_lsq_new\",\n",
    "    \"recurrence_time_5.1_mlk_new\",\n",
    "    \"recurrence_time_5.2_lsq_new\",\n",
    "    \"recurrence_time_5.2_mlk_new\",\n",
    "    \"recurrence_time_5.3_lsq_new\",\n",
    "    \"recurrence_time_5.3_mlk_new\",\n",
    "    \"recurrence_time_5.4_lsq_new\",\n",
    "    \"recurrence_time_5.4_mlk_new\",\n",
    "    \"recurrence_time_5.5_lsq_new\",\n",
    "    \"recurrence_time_5.5_mlk_new\",\n",
    "    \"recurrence_time_5.6_lsq_new\",\n",
    "    \"recurrence_time_5.6_mlk_new\",\n",
    "    \"recurrence_time_5.7_lsq_new\",\n",
    "    \"recurrence_time_5.7_mlk_new\",\n",
    "    \"recurrence_time_5.8_lsq_new\",\n",
    "    \"recurrence_time_5.8_mlk_new\",\n",
    "    \"recurrence_time_5.9_lsq_new\",\n",
    "    \"recurrence_time_5.9_mlk_new\",\n",
    "    \"recurrence_time_6.0_lsq_new\",\n",
    "    \"recurrence_time_6.0_mlk_new\"\n",
    "]\n",
    "\n",
    "# Updated target variable\n",
    "target_column =  \"mean_magnitude_new\"\n",
    "\n",
    "# Remove NaN values\n",
    "df_clean = df[feature_columns + [target_column]].dropna()\n",
    "\n",
    "# Normalize data\n",
    "scaler_X = MinMaxScaler()\n",
    "scaler_y = MinMaxScaler()\n",
    "\n",
    "X_scaled = scaler_X.fit_transform(df_clean[feature_columns])\n",
    "y_scaled = scaler_y.fit_transform(df_clean[[target_column]])\n",
    "\n",
    "# Define sliding window function\n",
    "### instead of using the sliding window feature go for absolute method of forecasting.....\n",
    "#def create_sliding_windows(X, y, look_back=10):\n",
    "#    X_windows, y_windows = [], []\n",
    "#    for i in range(len(X) - look_back):\n",
    "#        X_windows.append(X[i:i+look_back])\n",
    "#        y_windows.append(y[i+look_back])\n",
    "#    return np.array(X_windows), np.array(y_windows)\n",
    "\n",
    "#look_back = 10  # Predicting based on past 10 time steps\n",
    "X_lstm, y_lstm = create_sliding_windows(X_scaled, y_scaled, look_back)\n",
    "\n",
    "# Train-test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_lstm, y_lstm, test_size=0.2, random_state=42, shuffle=False)\n",
    "\n",
    "# Build LSTM model\n",
    "model = Sequential([\n",
    "    LSTM(100, return_sequences=True, input_shape=(look_back, X_train.shape[2])),\n",
    "    Dropout(0.2),\n",
    "    LSTM(50, return_sequences=False),\n",
    "    Dense(25, activation='relu'),\n",
    "    Dense(1)  # Predicting a single magnitude value\n",
    "])\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "finalkernel",
   "language": "python",
   "name": "finalkernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
